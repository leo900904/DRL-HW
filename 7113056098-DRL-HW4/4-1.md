# HW4-1: Naive DQN for Static Mode - 作業報告

## 1. 作業目標與內容
本次作業要求實作並理解Naive DQN（Deep Q-Network）於簡單環境（static mode），並進一步加入Experience Replay Buffer，觀察其對訓練穩定性與效能的影響。主要步驟包括：
- 執行並理解Naive DQN訓練流程
- 加入經驗回放（Experience Replay Buffer）
- 測試模型於不同環境下的表現
- 撰寫理解報告

## 2. Naive DQN 實作與流程說明

### 2.1 環境與狀態空間
- 使用Gridworld環境（4x4），static模式下所有物件位置固定。
- 狀態以4x4x4的one-hot張量表示，展平成長度64的向量作為神經網路輸入。

### 2.2 DQN網路架構
- 輸入層：64維
- 隱藏層：150、100維，皆用ReLU激活
- 輸出層：4維，對應四個動作的Q值
- 損失函數：MSELoss
- 優化器：Adam

### 2.3 訓練流程
- 每回合初始化遊戲，取得初始狀態
- 以ε-greedy策略選擇動作（初期多探索，逐步減少ε）
- 執行動作，取得新狀態與回饋
- 計算目標Q值，更新網路參數
- 記錄loss，繪製訓練曲線

### 2.4 測試與評估
- 提供`test_model`函數，能自動測試模型於static、random等模式下的表現
- 評估指標：勝率、平均步數、累積獎勵

## 3. Experience Replay Buffer 實作與優勢

### 3.1 實作方式
- 使用`collections.deque`建立固定長度的經驗回放記憶體
- 每步將(state, action, reward, next_state, done)存入buffer
- 當buffer超過batch size時，隨機抽取minibatch進行訓練
- 有效打破資料相關性，提高樣本利用率

### 3.2 訓練流程差異
- 每回合不再僅用當前經驗更新，而是從buffer隨機抽取多筆經驗進行批次訓練
- 訓練更穩定，loss曲線更平滑，收斂速度提升

### 3.3 測試結果
- 加入Experience Replay後，模型在random模式下的勝率明顯提升，訓練過程更穩定
- Loss曲線由原本大幅波動變得平滑，收斂更快

## 4. 主要程式邏輯摘要
- `Gridworld`環境初始化與狀態處理
- DQN網路建構與訓練主迴圈（含ε-greedy策略）
- Experience Replay Buffer的建立與minibatch訓練
- 測試函數自動評估模型表現

## 5. 心得與反思
- Naive DQN雖能在簡單環境下學習，但容易受資料相關性影響，訓練不穩定
- 加入Experience Replay後，訓練效率與穩定性大幅提升，模型泛化能力更好
- 本次作業讓我更深入理解DQN核心流程與強化學習訓練的關鍵技巧

## 6. 參考資料
- 課程教材與範例程式
- PyTorch官方文件
- DQN原始論文
