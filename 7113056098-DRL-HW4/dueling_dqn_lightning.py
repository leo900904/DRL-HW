import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping
from pytorch_lightning.loggers import TensorBoardLogger
import matplotlib.pyplot as plt
from collections import deque
import random
import os
from datetime import datetime

# å°†åŠ¨ä½œçš„å­—æ¯ä¸æ•°å­—å¯¹åº”èµ·æ¥
action_set = {
    0: 'u',  # "0"ä»£è¡¨"å‘ä¸Š"
    1: 'd',  # "1"ä»£è¡¨"å‘ä¸‹"
    2: 'l',  # "2"ä»£è¡¨"å‘å·¦"
    3: 'r'   # "3"ä»£è¡¨"å‘å³"
}

# Dueling DQN æ¶æ„çš„ Lightning æ¨¡å—
class DuelingDQNLightning(pl.LightningModule):
    def __init__(self, input_size=64, hidden_size=150, output_size=4, 
                 learning_rate=1e-3, gamma=0.99, epsilon=1.0, epsilon_min=0.01, 
                 epsilon_decay=None, total_epochs=3000, sync_rate=5, 
                 memory_size=5000, batch_size=256, gradient_clip=1.0):
        super().__init__()
        self.save_hyperparameters()
        
        # ç‰¹å¾æå–ç½‘ç»œ - å¢åŠ æ›´å¤šå±‚
        self.feature_network = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU()
        )
        
        # ä»·å€¼æµ - å¢åŠ æ›´å¤šå•å…ƒ
        self.value_stream = nn.Sequential(
            nn.Linear(hidden_size, hidden_size//2),
            nn.ReLU(),
            nn.Linear(hidden_size//2, hidden_size//4),
            nn.ReLU(),
            nn.Linear(hidden_size//4, 1)
        )
        
        # ä¼˜åŠ¿æµ - å¢åŠ æ›´å¤šå•å…ƒ
        self.advantage_stream = nn.Sequential(
            nn.Linear(hidden_size, hidden_size//2),
            nn.ReLU(),
            nn.Linear(hidden_size//2, hidden_size//4),
            nn.ReLU(),
            nn.Linear(hidden_size//4, output_size)
        )
        
        # ç›®æ ‡ç½‘ç»œï¼ˆDuelingç»“æ„ï¼‰- å¢åŠ æ›´å¤šå±‚
        self.target_feature_network = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU()
        )
        
        self.target_value_stream = nn.Sequential(
            nn.Linear(hidden_size, hidden_size//2),
            nn.ReLU(),
            nn.Linear(hidden_size//2, hidden_size//4),
            nn.ReLU(),
            nn.Linear(hidden_size//4, 1)
        )
        
        self.target_advantage_stream = nn.Sequential(
            nn.Linear(hidden_size, hidden_size//2),
            nn.ReLU(),
            nn.Linear(hidden_size//2, hidden_size//4),
            nn.ReLU(),
            nn.Linear(hidden_size//4, output_size)
        )
        
        # åˆå§‹åŒ–ç›®æ ‡ç½‘ç»œæƒé‡
        self._sync_target_network()
        
        # ç»éªŒå›æ”¾ç¼“å†²åŒº
        self.memory = deque(maxlen=memory_size)
        self.batch_size = batch_size
        
        # è®­ç»ƒå‚æ•°
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_min = epsilon_min
        self.total_epochs = total_epochs
        self.sync_rate = sync_rate  # ç›®æ ‡ç½‘ç»œåŒæ­¥é¢‘ç‡
        self.gradient_clip = gradient_clip  # æ¢¯åº¦è£å‰ªå€¼
        
        # çº¿æ€§è¡°å‡
        self.epsilon_decay = (epsilon - epsilon_min) / total_epochs if epsilon_decay is None else epsilon_decay
        
        # æŒ‡æ ‡è¿½è¸ª
        self.training_step_outputs = []
        self.episode_rewards = []
        self.episode_lengths = []
        self.success_rates = []
        self.learning_rate = learning_rate
        
        # è®­ç»ƒæ­¥æ•°è®¡æ•°å™¨
        self.training_steps = 0
    
    def forward(self, x):
        features = self.feature_network(x)
        values = self.value_stream(features)
        advantages = self.advantage_stream(features)
        # Dueling DQNçš„Qå€¼è®¡ç®—ï¼šQ(s,a) = V(s) + (A(s,a) - mean(A(s,:)))
        return values + (advantages - advantages.mean(dim=1, keepdim=True))
    
    def target_forward(self, x):
        features = self.target_feature_network(x)
        values = self.target_value_stream(features)
        advantages = self.target_advantage_stream(features)
        return values + (advantages - advantages.mean(dim=1, keepdim=True))
    
    def _sync_target_network(self):
        # å°†å½“å‰ç½‘ç»œçš„æƒé‡åŒæ­¥åˆ°ç›®æ ‡ç½‘ç»œ
        self.target_feature_network.load_state_dict(self.feature_network.state_dict())
        self.target_value_stream.load_state_dict(self.value_stream.state_dict())
        self.target_advantage_stream.load_state_dict(self.advantage_stream.state_dict())
    
    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=200, 
            min_lr=1e-5
        )
        return {
            "optimizer": optimizer,
            "lr_scheduler": {
                "scheduler": scheduler,
                "monitor": "train_loss",
                "frequency": 100,
                "interval": "step"
            }
        }
    
    # æ·»åŠ è·å–ä¼˜åŒ–å™¨çš„æ–¹æ³•
    def get_optimizer(self):
        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)
    
    # æ·»åŠ è·å–å­¦ä¹ ç‡è°ƒåº¦å™¨çš„æ–¹æ³•
    def get_scheduler(self, optimizer):
        return torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=200, 
            min_lr=1e-5
        )
    
    def training_step(self, batch, batch_idx):
        # è§£åŒ…æ‰¹æ¬¡æ•°æ®
        states, actions, rewards, next_states, dones = batch
        
        # ä½¿ç”¨å½“å‰ç½‘ç»œè®¡ç®—Qå€¼
        current_q_values = self(states)
        
        # Double DQNï¼šä½¿ç”¨å½“å‰ç½‘ç»œé€‰æ‹©åŠ¨ä½œï¼Œä½¿ç”¨ç›®æ ‡ç½‘ç»œä¼°è®¡Qå€¼
        with torch.no_grad():
            next_q_values = self(next_states)
            best_actions = torch.argmax(next_q_values, dim=1)
            next_q_values_target = self.target_forward(next_states)
            max_next_q = next_q_values_target.gather(1, best_actions.unsqueeze(1)).squeeze(1)
        
        # è®¡ç®—ç›®æ ‡Qå€¼
        expected_q_values = rewards + (1 - dones.float()) * self.gamma * max_next_q
        
        # è·å–æ‰€é€‰åŠ¨ä½œçš„Qå€¼
        actions_q_values = current_q_values.gather(1, actions.long().unsqueeze(1)).squeeze(1)
        
        # è®¡ç®—æŸå¤±ï¼Œä½¿ç”¨HuberæŸå¤±å¢å¼ºç¨³å®šæ€§
        loss = F.smooth_l1_loss(actions_q_values, expected_q_values)
        
        # è®°å½•æŒ‡æ ‡ - ä¿®æ”¹ä¸ºä¸ä¾èµ–Trainerçš„æ–¹å¼
        # self.log('train_loss', loss, prog_bar=True)
        
        self.training_steps += 1
        if self.training_steps % self.sync_rate == 0:
            self._sync_target_network()
        
        return loss
    
    def get_action(self, state, evaluate=False):
        # ç”¨äºé€‰æ‹©åŠ¨ä½œçš„æ–¹æ³•
        if not evaluate and random.random() < self.epsilon:
            return random.randint(0, 3)  # æ¢ç´¢
        
        with torch.no_grad():
            q_values = self(state)
            return torch.argmax(q_values).item()  # åˆ©ç”¨

    def update_epsilon(self):
        # æ›´æ–°epsilonå€¼ï¼ˆÎµ-è´ªå¿ƒç­–ç•¥ï¼‰
        if self.epsilon > self.epsilon_min:
            self.epsilon -= self.epsilon_decay
            return max(self.epsilon, self.epsilon_min)
        return self.epsilon

    def add_experience(self, state, action, reward, next_state, done):
        # æ·»åŠ ç»éªŒåˆ°å›æ”¾ç¼“å†²åŒº
        self.memory.append((state, torch.tensor([action]), 
                           torch.tensor([reward]), next_state, 
                           torch.tensor([done], dtype=torch.float)))

# è®­ç»ƒå‡½æ•°
def train_dueling_dqn(gridworld_class, epochs=3000, mode='random', hidden_size=256,
                     lr=5e-4, batch_size=128, memory_size=10000, use_tensorboard=False,
                     wall_penalty=-5.0, tensorboard_name="dueling_dqn"):
    from Gridworld import Gridworld  # å¯¼å…¥åœ¨è®­ç»ƒå‡½æ•°å†…éƒ¨ï¼Œä»¥ç¡®ä¿å¯ä»¥è®¿é—®
    
    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    os.makedirs('models', exist_ok=True)
    os.makedirs('plots', exist_ok=True)
    if use_tensorboard:
        os.makedirs('logs', exist_ok=True)
        logger = TensorBoardLogger("logs", name=tensorboard_name)
    else:
        logger = None
    
    # åˆå§‹åŒ–æ¨¡å‹å’Œå›è°ƒ
    model = DuelingDQNLightning(
        input_size=64,
        hidden_size=hidden_size,
        output_size=4,
        learning_rate=lr,
        gamma=0.99,  # æ›´é«˜çš„æŠ˜æ‰£å› å­
        epsilon=1.0,
        epsilon_min=0.01,  # æ›´ä½çš„æœ€å°æ¢ç´¢ç‡
        total_epochs=epochs,
        memory_size=memory_size,
        batch_size=batch_size,
        sync_rate=5  # æ›´é¢‘ç¹åœ°åŒæ­¥ç›®æ ‡ç½‘ç»œ
    )
    
    # åˆ›å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    optimizer = model.get_optimizer()
    scheduler = model.get_scheduler(optimizer)
    
    # ç§»åŠ¨æ–¹å‘å‘é‡ï¼ˆç”¨äºå¢™ä½“æ£€æµ‹ï¼‰
    move_pos = [(-1,0), (1,0), (0,-1), (0,1)]  # u,d,l,r
    
    # è®­ç»ƒç›‘æ§å˜é‡
    log_epochs = []
    log_loss = []
    log_success = []
    log_epsilon = []
    log_avg_reward = []
    
    max_steps_per_episode = 100  # æ¯ä¸ªepisodeçš„æœ€å¤§æ­¥æ•°é™åˆ¶
    
    for epoch in range(epochs):
        # åˆ›å»ºæ¸¸æˆç¯å¢ƒ
        game = gridworld_class(size=4, mode=mode)
        state = torch.from_numpy(game.board.render_np().reshape(1,64) + np.random.rand(1,64)/100.0).float()
        
        done = False
        total_reward = 0
        episode_length = 0
        step_loss = 0
        
        # å•ä¸ªepisodeå¾ªç¯
        while not done and episode_length < max_steps_per_episode:
            # è·å–åŠ¨ä½œ
            action_idx = model.get_action(state)
            
            # æ£€æŸ¥æ˜¯å¦æ’å¢™
            hit_wall = game.validateMove('Player', move_pos[action_idx]) == 1
            
            # æ‰§è¡ŒåŠ¨ä½œ
            action = action_set[action_idx]
            game.makeMove(action)
            
            # è·å–æ–°çŠ¶æ€å’Œå¥–åŠ±
            next_state = torch.from_numpy(game.board.render_np().reshape(1,64) + np.random.rand(1,64)/100.0).float()
            
            # æ”¹è¿›å¥–åŠ±è®¾è®¡
            raw_reward = game.reward()
            if hit_wall:
                reward = wall_penalty  # æ’å¢™æƒ©ç½š
            elif raw_reward > 0:
                # ç»™æ­£å¥–åŠ±åŠ ä¸Šé¢å¤–çš„å¥–åŠ±ä»¥é¼“åŠ±æ‰¾åˆ°ç›®æ ‡
                reward = raw_reward + 5.0
            elif raw_reward < 0 and raw_reward != -1:
                # ç»™è´Ÿå¥–åŠ±ä¿æŒåŸæ ·ï¼Œé¼“åŠ±é¿å¼€é™·é˜±
                reward = raw_reward
            else:
                # æ¯èµ°ä¸€æ­¥éƒ½æœ‰å°æƒ©ç½šï¼Œé¼“åŠ±å°½å¿«æ‰¾åˆ°ç›®æ ‡
                reward = -0.1
                
            game_over = raw_reward != -1  # æ¸¸æˆæ˜¯å¦ç»“æŸ
            
            # å­˜å‚¨ç»éªŒ
            model.add_experience(state, action_idx, reward, next_state, game_over)
            
            # å¦‚æœæœ‰è¶³å¤Ÿçš„æ ·æœ¬ï¼Œè¿›è¡Œè®­ç»ƒ
            if len(model.memory) >= batch_size:
                # ä»è®°å¿†ä¸­éšæœºé‡‡æ ·
                minibatch = random.sample(model.memory, batch_size)
                
                # å¤„ç†æ‰¹æ¬¡æ•°æ®
                state_batch = torch.cat([s for s, _, _, _, _ in minibatch])
                action_batch = torch.cat([a for _, a, _, _, _ in minibatch])
                reward_batch = torch.cat([r for _, _, r, _, _ in minibatch])
                next_state_batch = torch.cat([ns for _, _, _, ns, _ in minibatch])
                done_batch = torch.cat([d for _, _, _, _, d in minibatch])
                
                # è®­ç»ƒæ­¥éª¤
                batch_data = (state_batch, action_batch, reward_batch, next_state_batch, done_batch)
                loss = model.training_step(batch_data, 0)
                
                # æ¢¯åº¦ä¼˜åŒ– - ä½¿ç”¨æ ‡å‡†çš„PyTorch API
                optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), model.gradient_clip)
                optimizer.step()
                
                # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
                scheduler.step(loss.item())
                
                step_loss = loss.item()
            
            # æ›´æ–°çŠ¶æ€
            state = next_state
            total_reward += reward
            episode_length += 1
            
            # æ£€æŸ¥æ¸¸æˆæ˜¯å¦ç»“æŸ
            done = game_over
        
        # è®°å½•æœ¬æ¬¡episodeçš„ç»“æœ
        model.episode_rewards.append(total_reward)
        model.episode_lengths.append(episode_length)
        
        # æ›´æ–°Îµå€¼
        model.update_epsilon()
        
        # æ¯éš”ä¸€å®šé—´éš”è¯„ä¼°æ¨¡å‹
        if epoch % 10 == 0:
            success_rate = test_model(model, gridworld_class, mode=mode, test_episodes=20)
            model.success_rates.append(success_rate)
            
            # è®¡ç®—è¿‡å»10ä¸ªepisodeçš„å¹³å‡å¥–åŠ±
            avg_reward = np.mean(model.episode_rewards[-10:]) if len(model.episode_rewards) >= 10 else np.mean(model.episode_rewards)
            
            print(f"Episode {epoch}, Reward: {total_reward:.1f}, Length: {episode_length}, "
                  f"Success: {success_rate:.1f}%, Epsilon: {model.epsilon:.3f}, "
                  f"Loss: {step_loss:.5f}")
            
            # è®°å½•æŒ‡æ ‡
            log_epochs.append(epoch)
            log_loss.append(step_loss)
            log_success.append(success_rate)
            log_epsilon.append(model.epsilon)
            log_avg_reward.append(avg_reward)
            
            # æ¯100ä¸ªepochä¿å­˜æ¨¡å‹
            if epoch % 100 == 0 and epoch > 0:
                torch.save(model.state_dict(), f'models/dueling_dqn_epoch_{epoch}.pth')
    
    # è®­ç»ƒç»“æŸåä¿å­˜æœ€ç»ˆæ¨¡å‹
    torch.save(model.state_dict(), 'models/dueling_dqn_final.pth')
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plot_training_curves(log_epochs, log_loss, log_success, log_epsilon, log_avg_reward)
    
    return model

# æµ‹è¯•å‡½æ•°
def test_model(model, gridworld_class, mode='random', test_episodes=100, visualize=False):
    from Gridworld import Gridworld
    wins = 0
    rewards = []
    steps = []
    
    for _ in range(test_episodes):
        game = gridworld_class(size=4, mode=mode)
        state = torch.from_numpy(game.board.render_np().reshape(1,64) + np.random.rand(1,64)/100.0).float()
        done = False
        total_reward = 0
        step_count = 0
        max_steps = 50  # é˜²æ­¢æ— é™å¾ªç¯
        
        if visualize:
            print("\nInitial state:")
            game.display()
        
        while not done and step_count < max_steps:
            # è¯„ä¼°æ¨¡å¼ä¸‹é€‰æ‹©åŠ¨ä½œ
            action_idx = model.get_action(state, evaluate=True)
            action = action_set[action_idx]
            
            # æ‰§è¡ŒåŠ¨ä½œ
            game.makeMove(action)
            
            if visualize:
                print(f"Step {step_count+1}, Action: {action}")
                game.display()
            
            # æ›´æ–°çŠ¶æ€
            state = torch.from_numpy(game.board.render_np().reshape(1,64) + np.random.rand(1,64)/100.0).float()
            reward = game.reward()
            total_reward += reward
            
            # æ£€æŸ¥æ¸¸æˆæ˜¯å¦ç»“æŸ
            done = reward != -1
            step_count += 1
            
            # æ£€æŸ¥æ˜¯å¦èƒœåˆ©
            if reward > 0:
                wins += 1
                if visualize:
                    print("Win! ğŸ‰")
                break
        
        rewards.append(total_reward)
        steps.append(step_count)
        
        if visualize and not done:
            print("Max steps reached without result.")
    
    win_rate = 100.0 * wins / test_episodes
    avg_reward = np.mean(rewards)
    avg_steps = np.mean(steps)
    
    if visualize:
        print(f"\nTest Results ({test_episodes} episodes):")
        print(f"Win Rate: {win_rate:.1f}%")
        print(f"Average Reward: {avg_reward:.2f}")
        print(f"Average Steps: {avg_steps:.2f}")
    
    return win_rate

# ç»˜åˆ¶è®­ç»ƒæ›²çº¿
def plot_training_curves(epochs, losses, success_rates, epsilons, rewards):
    # è®¾ç½®ç»˜å›¾æ ·å¼
    plt.style.use('ggplot')
    
    # åˆ›å»ºåŒ…å«4ä¸ªå­å›¾çš„å›¾è¡¨
    fig, axs = plt.subplots(2, 2, figsize=(20, 12))
    fig.suptitle('Dueling DQN with PyTorch Lightning - è®­ç»ƒæŒ‡æ ‡', fontsize=20, fontweight='bold')
    
    # 1. æŸå¤±æ›²çº¿
    axs[0, 0].plot(epochs, losses, 'b-', linewidth=2, label='è®­ç»ƒæŸå¤±')
    # è®¡ç®—ç§»åŠ¨å¹³å‡çº¿ä»¥æ˜¾ç¤ºè¶‹åŠ¿
    window_size = min(20, len(losses))
    if window_size > 0:
        moving_avg = [np.mean(losses[max(0, i-window_size):i+1]) for i in range(len(losses))]
        axs[0, 0].plot(epochs, moving_avg, 'r-', linewidth=2, label='ç§»åŠ¨å¹³å‡ (çª—å£={})'.format(window_size))
    axs[0, 0].set_title('è®­ç»ƒæŸå¤±æ›²çº¿', fontsize=16, fontweight='bold')
    axs[0, 0].set_xlabel('è®­ç»ƒè½®æ¬¡', fontsize=14)
    axs[0, 0].set_ylabel('æŸå¤±å€¼', fontsize=14)
    axs[0, 0].grid(True, linestyle='--', alpha=0.7)
    axs[0, 0].legend(fontsize=12)
    # æ ‡è®°æœ€ä½æŸå¤±ç‚¹
    if len(losses) > 0:
        min_loss_idx = np.argmin(losses)
        min_loss = losses[min_loss_idx]
        axs[0, 0].scatter([epochs[min_loss_idx]], [min_loss], color='green', s=100, zorder=5)
        axs[0, 0].annotate(f'æœ€ä½æŸå¤±: {min_loss:.4f}',
                          xy=(epochs[min_loss_idx], min_loss),
                          xytext=(epochs[min_loss_idx], min_loss*1.5),
                          arrowprops=dict(facecolor='black', shrink=0.05),
                          fontsize=12)
    
    # 2. æˆåŠŸç‡æ›²çº¿
    axs[0, 1].plot(epochs, success_rates, 'g-', linewidth=2, label='æˆåŠŸç‡')
    # æ·»åŠ æ°´å¹³çº¿æ ‡è®°å…³é”®å€¼
    axs[0, 1].axhline(y=50, color='r', linestyle='--', alpha=0.5, label='50%åŸºå‡†')
    axs[0, 1].set_title('æˆåŠŸç‡æ›²çº¿', fontsize=16, fontweight='bold')
    axs[0, 1].set_xlabel('è®­ç»ƒè½®æ¬¡', fontsize=14)
    axs[0, 1].set_ylabel('æˆåŠŸç‡ (%)', fontsize=14)
    axs[0, 1].set_ylim(0, 105)  # ç¡®ä¿yè½´ä»0åˆ°100ä»¥ä¸Šï¼Œç•™å‡ºæ ‡æ³¨ç©ºé—´
    axs[0, 1].grid(True, linestyle='--', alpha=0.7)
    axs[0, 1].legend(fontsize=12)
    # æ ‡è®°æœ€é«˜æˆåŠŸç‡ç‚¹
    if len(success_rates) > 0:
        max_success_idx = np.argmax(success_rates)
        max_success = success_rates[max_success_idx]
        axs[0, 1].scatter([epochs[max_success_idx]], [max_success], color='red', s=100, zorder=5)
        axs[0, 1].annotate(f'æœ€é«˜æˆåŠŸç‡: {max_success:.1f}%',
                          xy=(epochs[max_success_idx], max_success),
                          xytext=(epochs[max_success_idx], max_success - 20),
                          arrowprops=dict(facecolor='black', shrink=0.05),
                          fontsize=12)
    
    # 3. Epsilonè¡°å‡æ›²çº¿
    axs[1, 0].plot(epochs, epsilons, 'r-', linewidth=2, label='Epsilonå€¼')
    axs[1, 0].set_title('Epsilonæ¢ç´¢ç‡è¡°å‡æ›²çº¿', fontsize=16, fontweight='bold')
    axs[1, 0].set_xlabel('è®­ç»ƒè½®æ¬¡', fontsize=14)
    axs[1, 0].set_ylabel('Epsilonå€¼', fontsize=14)
    axs[1, 0].set_ylim(0, 1.1)  # ç¡®ä¿yè½´ä»0åˆ°1.1
    axs[1, 0].grid(True, linestyle='--', alpha=0.7)
    axs[1, 0].legend(fontsize=12)
    
    # 4. å¹³å‡å¥–åŠ±æ›²çº¿
    axs[1, 1].plot(epochs, rewards, 'y-', linewidth=2, label='å¹³å‡å¥–åŠ±')
    # è®¡ç®—ç§»åŠ¨å¹³å‡çº¿ä»¥æ˜¾ç¤ºè¶‹åŠ¿
    if len(rewards) > 0:
        window_size = min(20, len(rewards))
        moving_avg = [np.mean(rewards[max(0, i-window_size):i+1]) for i in range(len(rewards))]
        axs[1, 1].plot(epochs, moving_avg, 'c-', linewidth=2, label='ç§»åŠ¨å¹³å‡ (çª—å£={})'.format(window_size))
    axs[1, 1].axhline(y=0, color='k', linestyle='-', alpha=0.3)  # é›¶çº¿
    axs[1, 1].set_title('å¹³å‡å¥–åŠ±æ›²çº¿', fontsize=16, fontweight='bold')
    axs[1, 1].set_xlabel('è®­ç»ƒè½®æ¬¡', fontsize=14)
    axs[1, 1].set_ylabel('å¹³å‡å¥–åŠ±', fontsize=14)
    axs[1, 1].grid(True, linestyle='--', alpha=0.7)
    axs[1, 1].legend(fontsize=12)
    
    # æ·»åŠ è®­ç»ƒä¿¡æ¯æ ‡æ³¨
    # åœ¨å›¾è¡¨åº•éƒ¨æ·»åŠ è®­ç»ƒå‚æ•°ä¿¡æ¯
    fig.text(0.5, 0.01, 
            f'è®­ç»ƒå‚æ•°: è½®æ¬¡={len(epochs)}, æœ€ç»ˆEpsilon={epsilons[-1]:.3f}, æœ€é«˜æˆåŠŸç‡={max(success_rates):.1f}%', 
            ha='center', fontsize=12, bbox=dict(facecolor='white', alpha=0.5))
    
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.savefig('plots/dueling_dqn_training_metrics.png', dpi=300)
    plt.close()
    
    # å•ç‹¬ä¿å­˜é«˜åˆ†è¾¨ç‡å›¾è¡¨
    metrics = {
        'loss': (epochs, losses, 'è®­ç»ƒæŸå¤±æ›²çº¿', 'è®­ç»ƒè½®æ¬¡', 'æŸå¤±å€¼'),
        'success_rate': (epochs, success_rates, 'æˆåŠŸç‡æ›²çº¿', 'è®­ç»ƒè½®æ¬¡', 'æˆåŠŸç‡ (%)'),
        'epsilon': (epochs, epsilons, 'Epsilonæ¢ç´¢ç‡è¡°å‡æ›²çº¿', 'è®­ç»ƒè½®æ¬¡', 'Epsilonå€¼'),
        'reward': (epochs, rewards, 'å¹³å‡å¥–åŠ±æ›²çº¿', 'è®­ç»ƒè½®æ¬¡', 'å¹³å‡å¥–åŠ±')
    }
    
    for name, (x, y, title, xlabel, ylabel) in metrics.items():
        plt.figure(figsize=(12, 8))
        plt.title(title, fontsize=18, fontweight='bold')
        plt.plot(x, y, linewidth=2.5)
        
        # å¯¹äºæŸå¤±å’Œå¥–åŠ±ï¼Œæ·»åŠ ç§»åŠ¨å¹³å‡çº¿
        if name in ['loss', 'reward'] and len(y) > 0:
            window_size = min(20, len(y))
            moving_avg = [np.mean(y[max(0, i-window_size):i+1]) for i in range(len(y))]
            plt.plot(x, moving_avg, 'r-', linewidth=2, alpha=0.7, label='ç§»åŠ¨å¹³å‡ (çª—å£={})'.format(window_size))
            plt.legend(fontsize=12)
        
        # å¯¹äºæˆåŠŸç‡å›¾ï¼Œæ·»åŠ 50%åŸºå‡†çº¿å’Œæ ‡è®°æœ€é«˜ç‚¹
        if name == 'success_rate' and len(y) > 0:
            plt.axhline(y=50, color='r', linestyle='--', alpha=0.5)
            max_idx = np.argmax(y)
            max_val = y[max_idx]
            plt.scatter([x[max_idx]], [max_val], color='red', s=100, zorder=5)
            plt.annotate(f'æœ€é«˜æˆåŠŸç‡: {max_val:.1f}%',
                         xy=(x[max_idx], max_val),
                         xytext=(x[max_idx], max_val - 20),
                         arrowprops=dict(facecolor='black', shrink=0.05),
                         fontsize=12)
            plt.ylim(0, 105)
        
        plt.xlabel(xlabel, fontsize=14)
        plt.ylabel(ylabel, fontsize=14)
        plt.grid(True, linestyle='--', alpha=0.7)
        plt.tight_layout()
        plt.savefig(f'plots/dueling_dqn_{name}.png', dpi=300)
        plt.close()
    
    # åˆ›å»ºé¢å¤–çš„ç»„åˆå¯¹æ¯”å›¾è¡¨
    plt.figure(figsize=(14, 10))
    plt.subplot(2, 1, 1)
    plt.title('æˆåŠŸç‡ vs. æŸå¤±', fontsize=16, fontweight='bold')
    plt.plot(epochs, success_rates, 'g-', linewidth=2, label='æˆåŠŸç‡')
    plt.ylabel('æˆåŠŸç‡ (%)', fontsize=14)
    plt.legend(loc='upper left', fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.7)
    
    ax2 = plt.twinx()
    ax2.plot(epochs, losses, 'b-', linewidth=2, alpha=0.6, label='æŸå¤±')
    ax2.set_ylabel('æŸå¤±å€¼', fontsize=14)
    ax2.legend(loc='upper right', fontsize=12)
    
    plt.subplot(2, 1, 2)
    plt.title('æˆåŠŸç‡ vs. Epsilon', fontsize=16, fontweight='bold')
    plt.plot(epochs, success_rates, 'g-', linewidth=2, label='æˆåŠŸç‡')
    plt.xlabel('è®­ç»ƒè½®æ¬¡', fontsize=14)
    plt.ylabel('æˆåŠŸç‡ (%)', fontsize=14)
    plt.legend(loc='upper left', fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.7)
    
    ax4 = plt.twinx()
    ax4.plot(epochs, epsilons, 'r-', linewidth=2, alpha=0.6, label='Epsilon')
    ax4.set_ylabel('Epsilonå€¼', fontsize=14)
    ax4.legend(loc='upper right', fontsize=12)
    
    plt.tight_layout()
    plt.savefig('plots/dueling_dqn_combined_metrics.png', dpi=300)
    plt.close()

if __name__ == "__main__":
    from Gridworld import Gridworld
    
    # ä¸»è®­ç»ƒå¾ªç¯
    print("å¼€å§‹è®­ç»ƒ Dueling DQN ä½¿ç”¨ PyTorch Lightning...")
    model = train_dueling_dqn(
        Gridworld,
        epochs=3000, 
        mode='random',
        hidden_size=256,
        lr=3e-4,
        batch_size=128,
        memory_size=10000,
        wall_penalty=-5.0
    )
    
    # æœ€ç»ˆæµ‹è¯•
    print("\nå¼€å§‹è¿›è¡Œæœ€ç»ˆæµ‹è¯•...")
    final_success_rate = test_model(model, Gridworld, mode='random', test_episodes=100, visualize=True)
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    with open(f'results_{timestamp}.txt', 'w') as f:
        f.write(f"æœ€ç»ˆæˆåŠŸç‡: {final_success_rate:.1f}%\n")
        f.write(f"æœ€ç»ˆEpsilonå€¼: {model.epsilon:.3f}\n")
        f.write(f"å¹³å‡Episodeé•¿åº¦: {np.mean(model.episode_lengths):.1f}\n")
        f.write(f"å¹³å‡Episodeå¥–åŠ±: {np.mean(model.episode_rewards):.1f}\n") 